{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHmWwQfJoC8c"
   },
   "source": [
    "# **Movie Recommendation System**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pXuBHOOoEpp"
   },
   "source": [
    "## Data Collection**\n",
    "\n",
    "The dataset has been obtained from Grouplens.\n",
    "\n",
    "Link : https://grouplens.org/datasets/movielens/20m/\n",
    "\n",
    "This dataset (ml-20m) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications across 27278 movies. These data were created by 138493 users between January 09, 1995 and March 31, 2015. This dataset was generated on October 17, 2016.\n",
    "\n",
    "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "The data are contained in the files genome-scores.csv, genome-tags.csv, links.csv, movies.csv, ratings.csv and tags.csv. \n",
    "\n",
    "For our objective, we would be using \"ratings.csv\" and \"movies.csv\" data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "59d_tqVu4xsH"
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Feh_60uM5Fot"
   },
   "outputs": [],
   "source": [
    "# Setting up some parameters for the workbook\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (25,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQCU54OoU5NW",
    "outputId": "777b7ae2-9a77-488f-f7d8-5ea5880b21be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Using cached scikit_surprise-1.1.4.tar.gz (154 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\smabi\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\smabi\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\smabi\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.10.1)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (pyproject.toml): started\n",
      "  Building wheel for scikit-surprise (pyproject.toml): finished with status 'error'\n",
      "Failed to build scikit-surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for scikit-surprise (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [117 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*.so' found under directory 'surprise'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\smabi\\AppData\\Local\\Temp\\pip-build-env-jaf0ah_8\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:218: _Warning: Package 'surprise.prediction_algorithms' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'surprise.prediction_algorithms' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'surprise.prediction_algorithms' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'surprise.prediction_algorithms' to be distributed and are\n",
      "          already explicitly excluding 'surprise.prediction_algorithms' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (scikit-surprise)\n"
     ]
    }
   ],
   "source": [
    "# !pip install fuzzywuzzy\n",
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XANhhXia5Fus"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Reader, Dataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaselineOnly\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from surprise import Reader, Dataset\n",
    "from surprise import BaselineOnly\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SlopeOne\n",
    "from surprise import SVD\n",
    "from surprise import SVDpp\n",
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BECPwpDC5F07"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZJygJn3oiyv"
   },
   "source": [
    "## **3. Data Preparation/Preprocessing**\n",
    "\n",
    "We will start with loading and familiarizing with the dataset so that we can prepare the data for Machine Learning (ML) modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoeyhQQEosRr"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "file_path = \"/content/drive/MyDrive/Colab Datasets/Movie Recommendation\"\n",
    "\n",
    "movie_ratings = pd.read_csv(file_path + \"/ratings.csv\")\n",
    "movies = pd.read_csv(file_path + \"/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPj1K_MgDv-S"
   },
   "outputs": [],
   "source": [
    "# Creating a newId for every movie to reduce the range of existing movieId\n",
    "\n",
    "movies[\"newId\"] = range(1, movies[\"movieId\"].nunique()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pu3vydusDuhZ"
   },
   "outputs": [],
   "source": [
    "# Converting the the UTC timestamp to Datetime\n",
    "movie_ratings[\"timestamp\"] = movie_ratings[\"timestamp\"].apply(lambda x: datetime.utcfromtimestamp(x).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# Merging the movies and ratings data files\n",
    "movie_ratings = movie_ratings.merge(movies, how=\"left\", on=\"movieId\")\n",
    "\n",
    "# Renaming the timestamp to date\n",
    "movie_ratings.rename(columns={\"timestamp\": \"date\"}, inplace=True)\n",
    "\n",
    "# Updating the movieId with the newId\n",
    "movie_ratings[\"movieId\"] = movie_ratings[\"newId\"]\n",
    "movies[\"movieId\"] = movies[\"newId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdHjouXUIB8H"
   },
   "outputs": [],
   "source": [
    "# Dropping the newId from the datasets\n",
    "movie_ratings.drop([\"newId\"], axis=1, inplace=True)\n",
    "movies.drop([\"newId\"], axis=1, inplace=True)\n",
    "\n",
    "# Sorting ratings based on date\n",
    "movie_ratings.sort_values(by = \"date\", inplace = True)\n",
    "movie_ratings.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "VxpAjFq85m3b",
    "outputId": "b90192b0-d3c2-4cdc-8d3a-592085b21b6a"
   },
   "outputs": [],
   "source": [
    "# Checking the features and no. of records in the dataset\n",
    "\n",
    "print(\"The number of records are : \", movie_ratings.shape[0])\n",
    "print(\"The number of features are : \", movie_ratings.shape[1])\n",
    "print(\"The list of features is : \", movie_ratings.columns)\n",
    "movie_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AuBVm-bMkyA"
   },
   "source": [
    "Observations:\n",
    "\n",
    "1. There are 20M+ records of the data.\n",
    "2. There are 6 features: userId, movieId, rating, date, title and genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWrEBqC7M_sj"
   },
   "source": [
    "### **3.1 Data Cleaning**\n",
    "\n",
    "We will begin with data cleaning such that we can handle missing values, outliers, rare values and drop the unnecessary features that do not carry useful information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJDLOxeu-j8K",
    "outputId": "9c7603df-1da3-4624-c0e3-802fb5f778ac"
   },
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "\n",
    "print(\"No. of duplicates records in the dataset : \", movie_ratings.columns.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEsm51F7N8BN"
   },
   "source": [
    "Observations:\n",
    "1. There are no duplicate records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQHakfwPND5n",
    "outputId": "67d13699-cb42-4e0d-9a72-6a79a4ed277d"
   },
   "outputs": [],
   "source": [
    "# Checking the columns' titles and datatypes\n",
    "\n",
    "movie_ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-pi8pklOP5C"
   },
   "source": [
    "#### **3.1.1 Handling Missing Values**\n",
    "\n",
    "Identifying the features that have some missing values and imputing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lL_GiN0OFjJ",
    "outputId": "b3a50cf0-50c8-4d8d-8e88-93f95907d3ab"
   },
   "outputs": [],
   "source": [
    "# Checking the number of missing values in data\n",
    "\n",
    "movie_ratings.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYDOEv7uOtDl"
   },
   "source": [
    "Observations:\n",
    "\n",
    "1. It looks like that the dataset is well maintained as we do not see any missing values, which is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGNdRpgMO8Tx"
   },
   "source": [
    "### **3.2 Exploratory Data Analysis**\n",
    "\n",
    "After the data cleaning steps, we can now perform EDA on the dataset to discover patterns and relationships that will help in understanding the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eI1SDI4PRfr"
   },
   "source": [
    "#### **3.2.1 Univariate Analysis**\n",
    "\n",
    "Analyzing each feature inidividually to gain insights from the data and discover any outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IzzjiZ7bOUmO",
    "outputId": "7e67569e-061d-45ff-9a21-4e0dc5c89103"
   },
   "outputs": [],
   "source": [
    "# Checking the feature \"userID\"\n",
    "\n",
    "total_users = len(np.unique(movie_ratings[\"userId\"]))\n",
    "print(\"The count of unique userID in the dataset is : \", total_users)\n",
    "print(\"The top 5 userID in the dataset are : \\n\", movie_ratings[\"userId\"].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5YZPCo4tvbM"
   },
   "source": [
    "Observations:\n",
    "\n",
    "1. \"userId\" are the Users that were selected at random for inclusion and their ids have been anonymized.\n",
    "2. There are 138K+ unique users in the dataset.\n",
    "3. userId 118205 has around 9K records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1q0t8e48UPpe",
    "outputId": "f46e74d2-0e53-4d16-c3c6-34e5e05aaac5"
   },
   "outputs": [],
   "source": [
    "# Checking the feature \"movieID\"\n",
    "\n",
    "total_movies = len(np.unique(movie_ratings[\"movieId\"]))\n",
    "print(\"The count of unique movieID in the dataset is : \", total_movies)\n",
    "print(\"The top 5 movieID in the dataset are : \\n\", movie_ratings[\"movieId\"].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rr1Gz-Ctx2gX"
   },
   "source": [
    "Observations:\n",
    "\n",
    "1. \"movieId\" represents the movies with at least one rating or tag in the dataset.\n",
    "2. There are close to 26K+ unique movies in the dataset.\n",
    "3. movieId 294, 353, 316 and 588 are few popular movies which has been rated over 60K times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IIIt4wtdNyi"
   },
   "outputs": [],
   "source": [
    "# Helper function to Change the numeric label in terms of Millions\n",
    "\n",
    "def changingLabels(number):\n",
    "\n",
    "    return str(number/10**6) + \"M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "id": "FM1u-zOzVQiT",
    "outputId": "f0a69fe1-ea83-41d5-e3a6-978a86ae4ecd"
   },
   "outputs": [],
   "source": [
    "# Checking the feature \"rating\"\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "fig, axes = plt.subplots(1, 1, figsize=(25, 5), sharey=True)\n",
    "\n",
    "sns.countplot(\"rating\", data=movie_ratings, ax=axes)\n",
    "axes.set_yticklabels([changingLabels(num) for num in axes.get_yticks()])\n",
    "for p in axes.patches:\n",
    "    axes.annotate('{}'.format(p.get_height()), (p.get_x()+0.2, p.get_height()+100))\n",
    "\n",
    "plt.tick_params(labelsize = 15)\n",
    "plt.title(\"Distribution of Ratings in the dataset\", fontsize = 20)\n",
    "plt.xlabel(\"Ratings\", fontsize = 10)\n",
    "plt.ylabel(\"Counts(in Millions)\", fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUhy3vwgyaci"
   },
   "source": [
    "Observations:\n",
    "\n",
    "1. The ratings given by users to movies lies in between 0.5 to 5.\n",
    "2. A high proportion of the movies have been rated 3, 3.5 or 4 by the users.\n",
    "3. The distribution of ratings look a bit left skewed as large proportion of ratings is in between 3 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GiL5J031WvkM",
    "outputId": "fc062fae-8f7d-4661-b087-f5991da0d69e"
   },
   "outputs": [],
   "source": [
    "# Checking the feature \"date\"\n",
    "\n",
    "print(\"The count of unique date in the dataset is : \", movie_ratings[\"date\"].nunique())\n",
    "print(\"The first rating was given on : \", movie_ratings[\"date\"].min())\n",
    "print(\"The latest rating was given on : \", movie_ratings[\"date\"].max())\n",
    "print(\"The top 5 date in the dataset are : \\n\", movie_ratings[\"date\"].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKW2vZENzIvR"
   },
   "source": [
    "Observations:\n",
    "\n",
    "1. There are ~7K unique dates when the ratings were given by a user to a movie.\n",
    "2. The first rating was given on 1995-01-09 and the latest rating was given on 2015-03-31.\n",
    "3. Around 91K+ ratings were observed on 2000-11-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ua2AI1OUW6Qw",
    "outputId": "efcf4787-8533-4d82-b171-e88544982bcb"
   },
   "outputs": [],
   "source": [
    "# Checking the feature \"title\"\n",
    "\n",
    "movie_list = movie_ratings[\"title\"].unique()\n",
    "print(\"The count of unique title in the dataset is : \", movie_ratings[\"title\"].nunique())\n",
    "print(\"The top 5 title in the dataset are : \\n\", movie_ratings[\"title\"].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_p8YjqM79lI"
   },
   "source": [
    "Observations:\n",
    "\n",
    "1. There are 26K+ unique movie titles in the dataset.\n",
    "2. Pulp Fiction, Forrest Gump, Shawshank Redemption and Silence of the Lambs are the top 4 movies in terms of no. of ratings received which are over 60K+ for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uj6XTZErXPrh",
    "outputId": "06d1d7c3-1062-4e39-9034-d040eea6fe3a"
   },
   "outputs": [],
   "source": [
    "# Extract unique Genres along with their count\n",
    "\n",
    "unique_genres = {}\n",
    "\n",
    "def ExtractGenres(x):\n",
    "    for g in x.split(\"|\"):\n",
    "        if g not in unique_genres.keys():\n",
    "            unique_genres[g] = 1\n",
    "        else:\n",
    "            unique_genres[g] = unique_genres[g] + 1\n",
    "\n",
    "movie_ratings[\"genres\"].apply(ExtractGenres)\n",
    "print(\"Genres Extracted from the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "xE1fM8wTauFS",
    "outputId": "76993af6-69c1-40b1-8e41-20cf5201f6f5"
   },
   "outputs": [],
   "source": [
    "# Visualizing the feature \"Genres\"\n",
    "\n",
    "genres_df = pd.DataFrame(list(unique_genres.items()))\n",
    "genres_df.columns = [\"Genre\", \"Count\"] \n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "fig, axes = plt.subplots(1, 1, figsize=(25, 8), sharey=True)\n",
    "\n",
    "sns.barplot(y=\"Count\", x=\"Genre\", data=genres_df, ax=axes)\n",
    "axes.set_yticklabels([changingLabels(num) for num in axes.get_yticks()])\n",
    "for p in axes.patches:\n",
    "    axes.annotate('{}'.format(int(p.get_height())), (p.get_x(), p.get_height()+100))\n",
    "\n",
    "plt.tick_params(labelsize = 15)\n",
    "plt.title(\"Distribution of Genres in the dataset\", fontsize = 20)\n",
    "plt.xlabel(\"Genres\", fontsize = 15)\n",
    "plt.xticks(rotation=60, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.ylabel(\"Counts (in Millions)\", fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ACrmXF68wZr"
   },
   "source": [
    "Observations:\n",
    "\n",
    "1. There are 19 different genres of movies while there are few whose genre has not been mentioned.\n",
    "2. Drama, Comedy, Action and Thriller are top 4 genres of movies present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "HZMqYP1wUPvy",
    "outputId": "3ac36635-9724-4ade-9090-3f7bfd29505d"
   },
   "outputs": [],
   "source": [
    "movie_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfT4KHdm9on5"
   },
   "source": [
    "#### **3.2.2 Train & test Splitting**\n",
    "\n",
    "Splitting the data into train and test sets before proceeding towards further EDA and Feature Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abdj3LNVUThD",
    "outputId": "075a057d-d98e-4ab6-c7b2-abd3a72dd262"
   },
   "outputs": [],
   "source": [
    "# Creating the train test set\n",
    "\n",
    "file_path = \"/content/drive/MyDrive/Colab Datasets/Movie Recommendation\"\n",
    "\n",
    "if not os.path.isfile(file_path + \"/TrainData.pkl\"):\n",
    "    print(\"Creating Train Data and saving it..\")\n",
    "    movie_ratings.iloc[:int(movie_ratings.shape[0] * 0.80)].to_pickle(file_path + \"/TrainData.pkl\")\n",
    "    Train_Data = pd.read_pickle(file_path + \"/TrainData.pkl\")\n",
    "    Train_Data.reset_index(drop = True, inplace = True)\n",
    "else:\n",
    "    print(\"Loading Train Data..\")\n",
    "    Train_Data = pd.read_pickle(file_path + \"/TrainData.pkl\")\n",
    "    Train_Data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "if not os.path.isfile(file_path + \"/TestData.pkl\"):\n",
    "    print(\"Creating Test Data and saving it..\")\n",
    "    movie_ratings.iloc[int(movie_ratings.shape[0] * 0.80):].to_pickle(file_path + \"/TestData.pkl\")\n",
    "    Test_Data = pd.read_pickle(file_path + \"/TestData.pkl\")\n",
    "    Test_Data.reset_index(drop = True, inplace = True)\n",
    "else:\n",
    "    print(\"Loading Test Data..\")\n",
    "    Test_Data = pd.read_pickle(file_path + \"/TestData.pkl\")\n",
    "    Test_Data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "npkvqmeHfXHu",
    "outputId": "7bc4db81-9a8f-4fb5-9b85-7fafa4fa38cf"
   },
   "outputs": [],
   "source": [
    "Train_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "IVeQT3u0Wqdo",
    "outputId": "746f8017-b9c6-4d87-ec52-65428628a117"
   },
   "outputs": [],
   "source": [
    "# Creating list of unique movies from Train Set\n",
    "\n",
    "movie_list_in_training = Train_Data.drop_duplicates(subset=[\"title\"], keep=\"first\")[[\"movieId\", \"title\", \"genres\"]]\n",
    "movie_list_in_training = movie_list_in_training.reset_index(drop=True)\n",
    "movie_list_in_training.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Movie Recommendation System.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
